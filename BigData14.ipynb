{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7edd493d-6fa8-403d-ad45-8d75fab1d942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/03 10:43:33 WARN Utils: Your hostname, Rama resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/12/03 10:43:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/03 10:43:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/03 10:43:49 WARN Instrumentation: [a5c0ee22] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:===========>                                               (1 + 4) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/03 10:43:53 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/03 10:43:53 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/03 10:43:53 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 37110)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/wleowleo/anaconda3/envs/spark-env/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15fd470-35d5-42b3-9586-02ee4bdd69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.26205794067705,4.087352270317018]\n",
      "Intercept: 11.568912738159003\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Dataset dengan DenseVector\n",
    "data = [\n",
    "    (1, Vectors.dense(2.0, 3.0), 0),\n",
    "    (2, Vectors.dense(1.0, 5.0), 1),\n",
    "    (3, Vectors.dense(2.5, 4.5), 1),\n",
    "    (4, Vectors.dense(3.0, 6.0), 0)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['ID', 'Features', 'Label'])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "print(\"Coefficients:\", model.coefficients)\n",
    "print(\"Intercept:\", model.intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34668db-258f-42a2-b9f0-3f570ece41d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 5) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 1.0])),\n",
    "    (2, Vectors.dense([5.0, 5.0])),\n",
    "    (3, Vectors.dense([10.0, 10.0])),\n",
    "    (4, Vectors.dense([15.0, 15.0]))\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['ID', 'Features'])\n",
    "\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers:\", centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ed87b3-2121-4052-aa19-e43c274b3f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- price: integer (nullable = true)\n",
      " |-- area: integer (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: integer (nullable = true)\n",
      " |-- stories: integer (nullable = true)\n",
      " |-- mainroad: string (nullable = true)\n",
      " |-- guestroom: string (nullable = true)\n",
      " |-- basement: string (nullable = true)\n",
      " |-- hotwaterheating: string (nullable = true)\n",
      " |-- airconditioning: string (nullable = true)\n",
      " |-- parking: integer (nullable = true)\n",
      " |-- prefarea: string (nullable = true)\n",
      " |-- furnishingstatus: string (nullable = true)\n",
      "\n",
      "+--------+----+--------+---------+-------+--------+---------+--------+---------------+---------------+-------+--------+----------------+\n",
      "|   price|area|bedrooms|bathrooms|stories|mainroad|guestroom|basement|hotwaterheating|airconditioning|parking|prefarea|furnishingstatus|\n",
      "+--------+----+--------+---------+-------+--------+---------+--------+---------------+---------------+-------+--------+----------------+\n",
      "|13300000|7420|       4|        2|      3|     yes|       no|      no|             no|            yes|      2|     yes|       furnished|\n",
      "|12250000|8960|       4|        4|      4|     yes|       no|      no|             no|            yes|      3|      no|       furnished|\n",
      "|12250000|9960|       3|        2|      2|     yes|       no|     yes|             no|             no|      2|     yes|  semi-furnished|\n",
      "|12215000|7500|       4|        2|      2|     yes|       no|     yes|             no|            yes|      3|     yes|       furnished|\n",
      "|11410000|7420|       4|        1|      2|     yes|      yes|     yes|             no|            yes|      2|      no|       furnished|\n",
      "+--------+----+--------+---------+-------+--------+---------+--------+---------------+---------------+-------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.Setup SparkSession\n",
    "# Membuat session Spark agar bisa menjalankan PySpark.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Homework\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#2. Load Dataset\n",
    "df = spark.read.csv(\"Housing.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f06f51-1f81-499c-bcfb-e7f2abb82ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   price|label|\n",
      "+--------+-----+\n",
      "|13300000|    1|\n",
      "|12250000|    1|\n",
      "|12250000|    1|\n",
      "|12215000|    1|\n",
      "|11410000|    1|\n",
      "|10850000|    1|\n",
      "|10150000|    1|\n",
      "|10150000|    1|\n",
      "| 9870000|    1|\n",
      "| 9800000|    1|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "median_price = df.approxQuantile(\"price\", [0.5], 0.0)[0]\n",
    "\n",
    "df = df.withColumn(\"label\", when(col(\"price\") > median_price, 1).otherwise(0))\n",
    "df.select(\"price\", \"label\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1e97dc-e74d-4e73-8f47-0aa42188e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Categorical → Indexed\n",
    "indexer = StringIndexer(inputCol=\"furnishingstatus\", outputCol=\"furnishingstatus_index\")\n",
    "\n",
    "# OneHot → Vector\n",
    "encoder = OneHotEncoder(inputCols=[\"furnishingstatus_index\"],\n",
    "                        outputCols=[\"furnishingstatus_vec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65732ace-4daa-4e7a-b575-908cff3389a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_cols = [\"area\", \"bedrooms\", \"bathrooms\", \"stories\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols + [\"furnishingstatus_vec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03cd5444-fd42-4954-a510-da7905abf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2479fd-0c44-4149-8945-c37bb2b76464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30be1f35-c371-4cb7-b7cf-08c00bb8a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d762ef8-5b44-4303-8218-a7ee089854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [20, 50, 100])\n",
    "             .addGrid(rf.maxDepth, [3, 5, 10])\n",
    "             .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1446cb3f-e0a2-4630-aa78-46a4f4863ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/12/04 08:47:26 WARN DAGScheduler: Broadcasting large task binary with size 1009.7 KiB\n",
      "25/12/04 08:47:26 WARN DAGScheduler: Broadcasting large task binary with size 1167.6 KiB\n",
      "25/12/04 08:47:26 WARN DAGScheduler: Broadcasting large task binary with size 1243.9 KiB\n",
      "25/12/04 08:47:46 WARN DAGScheduler: Broadcasting large task binary with size 1024.6 KiB\n",
      "25/12/04 08:47:46 WARN DAGScheduler: Broadcasting large task binary with size 1169.6 KiB\n",
      "25/12/04 08:47:47 WARN DAGScheduler: Broadcasting large task binary with size 1253.1 KiB\n",
      "25/12/04 08:48:02 WARN DAGScheduler: Broadcasting large task binary with size 1091.2 KiB\n",
      "25/12/04 08:48:02 WARN DAGScheduler: Broadcasting large task binary with size 1165.9 KiB\n"
     ]
    }
   ],
   "source": [
    "cv_model = cv.fit(train)\n",
    "best_model = cv_model.bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2479a614-bb83-46cf-8b69-be36cd56ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.8468169761273209\n"
     ]
    }
   ],
   "source": [
    "pred = best_model.transform(test)\n",
    "auc = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"AUC =\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1bb33ee-2183-4ef5-8a0c-f831d7de5478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best numTrees: 20\n",
      "Best maxDepth: 3\n"
     ]
    }
   ],
   "source": [
    "rf_best = best_model.stages[-1]   # RandomForest ada di stage terakhir\n",
    "\n",
    "print(\"Best numTrees:\", rf_best.getNumTrees)\n",
    "print(\"Best maxDepth:\", rf_best.getOrDefault(\"maxDepth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0792a71-ec8a-435d-b78a-a768275fd2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Importances ===\n",
      "area: 0.6586149502561843\n",
      "bedrooms: 0.18320256482455802\n",
      "bathrooms: 0.08375230345994504\n",
      "stories: 0.02520539002624911\n",
      "furnishingstatus_vec: 0.00307514731315197\n"
     ]
    }
   ],
   "source": [
    "importances = rf_best.featureImportances\n",
    "\n",
    "feature_names = numeric_cols + [\"furnishingstatus_vec\"]\n",
    "\n",
    "print(\"=== Feature Importances ===\")\n",
    "for name, importance in zip(feature_names, importances):\n",
    "    print(f\"{name}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2ff89-8144-457c-afd4-8f979eb0557c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark-env)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
